import { NextRequest, NextResponse } from 'next/server'
import {
  openai,
  AI_CONFIG,
  UNDERSTANDING_PROMPT,
  getExtractionPrompt,
} from '@/lib/ai/openai'
import {
  AI_TOOLS,
  validateEventsArgs,
  validateUrgentMessageArgs,
  validateHighlightArgs,
} from '@/lib/ai/tools'
import { incrementAiUsage, validateMessageLength } from '@/lib/ai/rate-limiter'
import { aiLogger } from '@/lib/ai/logger'

// Force dynamic rendering
export const dynamic = 'force-dynamic'
export const revalidate = 0

// Types for request/response
interface ChatMessage {
  role: 'system' | 'user' | 'assistant'
  content: string
}

interface AIAssistantRequest {
  messages: ChatMessage[]
  action?: 'initial' | 'select_type' | 'extract_data' | 'understand_message'
  context?: string // Optional context from understanding round
}

export async function POST(req: NextRequest) {
  let requestBody: AIAssistantRequest | null = null
  try {
    requestBody = await req.json()
    if (!requestBody) throw new Error('Request body is required')
    const { messages, action = 'extract_data', context } = requestBody

    // Handle initial greeting (no rate limit check - doesn't use GPT)
    if (action === 'initial') {
      aiLogger.logInitial()
      return NextResponse.json({
        success: true,
        message: `×©×œ×•×! ğŸ‘‹

××” ×ª×¨×¦×” ×œ×”×•×¡×™×£ ×œ××¢×¨×›×ª?

1ï¸âƒ£ **××™×¨×•×¢** - ××™×¨×•×¢ ×‘×œ×•×— ×”×©× ×” ×©×œ ×‘×™×ª ×”×¡×¤×¨
2ï¸âƒ£ **×”×•×“×¢×” ×“×—×•×¤×”** - ×”×•×“×¢×” ×©×ª×•×¦×’ ×‘×‘×× ×¨ ×‘×“×£ ×”×‘×™×ª
3ï¸âƒ£ **×”×“×’×©×”** - ×”×™×©×’/×¤×¨×¡/××™×¨×•×¢ ××™×•×—×“ ×œ×§×¨×•×¡×œ×ª ×“×£ ×”×‘×™×ª

×‘×—×¨ ××¤×©×¨×•×ª ×•××¡×‘×™×¨ ×œ×š ××” ×¦×¨×™×š ×œ××œ×.

ğŸ’¡ ××’×‘×œ×” ×™×•××™×ª: 50 ×©×™××•×©×™× ×‘×™×•× | ××§×¡×™××•× 1500 ×ª×•×•×™× ×œ×”×•×“×¢×”`,
      })
    }

    // Check rate limit for all GPT requests
    const rateLimitResult = await incrementAiUsage()

    // Log rate limit check
    aiLogger.logRateLimit({
      usageCount: rateLimitResult.stats.currentCount,
      dailyLimit: rateLimitResult.stats.dailyLimit,
      rateLimitReached: rateLimitResult.stats.limitReached,
    })

    if (!rateLimitResult.success || rateLimitResult.stats.limitReached) {
      aiLogger.logRateLimit({
        usageCount: rateLimitResult.stats.currentCount,
        dailyLimit: rateLimitResult.stats.dailyLimit,
        rateLimitReached: true,
      })
      return NextResponse.json({
        success: false,
        error: `×”×’×¢×ª ×œ××’×‘×œ×” ×”×™×•××™×ª ×©×œ 50 ×©×™××•×©×™× ğŸ˜”

× ×¡×” ×©×•×‘ ××—×¨ ××• ×¦×•×¨ ×§×©×¨ ×¢× ×”×× ×”×œ.

×©×™××•×©×™× ×”×™×•×: ${rateLimitResult.stats.currentCount}/${rateLimitResult.stats.dailyLimit}`,
        rateLimitReached: true,
        stats: rateLimitResult.stats,
      })
    }

    // Validate message length (400 chars max)
    const lastUserMessage = messages[messages.length - 1]?.content || ''
    const lengthValidation = validateMessageLength(lastUserMessage)

    if (!lengthValidation.valid) {
      return NextResponse.json({
        success: false,
        error: `×”×”×•×“×¢×” ××¨×•×›×” ××“×™ ğŸ“

××§×¡×™××•×: ${lengthValidation.maxLength} ×ª×•×•×™×
×”×”×•×“×¢×” ×©×œ×š: ${lengthValidation.length} ×ª×•×•×™×

× ×¡×” ×œ×§×¦×¨ ××ª ×”×”×•×“×¢×”.`,
        messageTooLong: true,
      })
    }

    // Handle type selection
    if (action === 'select_type') {
      const lastMessage = messages[messages.length - 1]
      const userInput = lastMessage.content.toLowerCase()

      if (userInput.includes('××™×¨×•×¢') || userInput === '1') {
        aiLogger.logTypeSelection(userInput, true)
        return NextResponse.json({
          success: true,
          message: `××¢×•×œ×”! ×‘×•× × ×™×¦×•×¨ ××™×¨×•×¢ ×—×“×© ğŸ“…

**×ª××¨ ××ª ×”××™×¨×•×¢ ×‘××©×¤×˜ ××—×“**, ×›×•×œ×œ:
â€¢ ×©× ×”××™×¨×•×¢
â€¢ ×ª××¨×™×š (×œ×“×•×’××”: 15 ×‘××¨×¥ ××• 15/03/2025)
â€¢ ×©×¢×” (×œ×“×•×’××”: 17:00 ××• 5 ××—×”×´×¦) - ××•×¤×¦×™×•× ×œ×™
â€¢ ××™×§×•× - ××•×¤×¦×™×•× ×œ×™

**×“×•×’××”:**
"××¡×™×‘×ª ×¤×•×¨×™× ×‘-15/03/2025 ×‘×©×¢×” 17:00 ×‘×’×Ÿ ×”×™×œ×“×™×"

××•:
"××¡×™×‘×ª ×¤×•×¨×™× ×‘15 ×‘××¨×¥"

ğŸ’¡ **××¤×©×¨ ×’× ××¡×¤×¨ ××™×¨×•×¢×™×!**
"××¡×™×‘×ª ×¤×•×¨×™× ×‘15 ×‘××¨×¥ ×•×’× ××¡×™×‘×ª ×¡×™×•× ×‘30 ×‘×™×•× ×™"`,
        })
      } else if (
        userInput.includes('×”×•×“×¢×”') ||
        userInput.includes('×“×—×•×£') ||
        userInput === '2'
      ) {
        aiLogger.logTypeSelection(userInput, true)
        return NextResponse.json({
          success: true,
          message: `××¢×•×œ×”! ×‘×•× × ×™×¦×•×¨ ×”×•×“×¢×” ×“×—×•×¤×” ğŸ“¢

**×ª××¨ ××ª ×”×”×•×“×¢×”**, ×›×•×œ×œ:
â€¢ ×ª×•×›×Ÿ ×”×”×•×“×¢×” (×‘×¢×‘×¨×™×ª)
â€¢ ×ª××¨×™×š ×¡×™×•× - **×—×•×‘×”!** (×¢×“ ××ª×™ ×œ×”×¦×™×’)
â€¢ ×¡×•×’ ×”×”×•×“×¢×” (×“×—×•×£/××™×“×¢/××–×”×¨×”/×—×•×œ×¦×” ×œ×‘× ×”) - ××•×¤×¦×™×•× ×œ×™

**×“×•×’×××•×ª:**
"×ª×–×›×•×¨×ª ×—×•×œ×¦×” ×œ×‘× ×” ×œ××—×¨ ×¢×“ ×ª××¨×™×š 20/03/2025"

"××™×¨×•×¢ ×“×—×•×£: ×‘×™×˜×•×œ ×œ×™××•×“×™× ××—×¨ ×‘×’×œ×œ ××–×’ ××•×•×™×¨, ×¢×“ 18/03"

"×—×’ ×—× ×•×›×” ×©××—! ğŸ• ×ª×¨××” ×”×•×“×¢×” ×–×• ×œ××©×š 5 ×™××™×"

**ğŸ’¡ ×˜×™×¤:** ××¤×©×¨ ×œ×”×©×ª××© ×‘×ª××¨×™×›×™× ×™×—×¡×™×™×:
â€¢ "5 ×™××™×", "×©×‘×•×¢", "×¢×“ ×¡×•×£ ×”×—×•×“×©"`,
        })
      } else if (
        userInput.includes('×”×“×’×©×”') ||
        userInput.includes('×”×™×©×’') ||
        userInput.includes('×¤×¨×¡') ||
        userInput === '3'
      ) {
        aiLogger.logTypeSelection(userInput, true)
        return NextResponse.json({
          success: true,
          message: `××¢×•×œ×”! ×‘×•× × ×™×¦×•×¨ ×”×“×’×©×” ××™×•×—×“×ª âœ¨

**×ª××¨ ××ª ×”×”×“×’×©×”**, ×›×•×œ×œ:
â€¢ **× ×•×©× ×•×¡×•×’:** ×”×™×©×’/×¡×¤×•×¨×˜/×¤×¨×¡/××™×¨×•×¢/×”×•×“×¢×”
â€¢ **×›×•×ª×¨×ª ×§×¦×¨×”:** ××” ×§×¨×”? (×œ××©×œ: "××§×•× ×¨××©×•×Ÿ ×‘××œ×™×¤×•×ª")
â€¢ **×ª×™××•×¨ ××¤×•×¨×˜:** ×¤×¨×˜×™× × ×•×¡×¤×™× ×¢×œ ×”×”×™×©×’/××™×¨×•×¢
â€¢ **×ª××¨×™×š:** ××ª×™ ×–×” ×§×¨×”? (×× ×œ× ×ª×¦×™×™×Ÿ - ××©××œ!)
â€¢ **×§×˜×’×•×¨×™×”:** (×›×“×•×¨×¡×œ, ×©×—×™×™×”, ××× ×•×ª ×•×›×•' - ××•×˜×•××˜×™ ×× ×œ× ×ª×¦×™×™×Ÿ)

**×“×•×’×××•×ª:**
"×”×™×©×’ ×‘×›×“×•×¨×¡×œ - ×–×›×™× ×• ×‘××§×•× ×”×¨××©×•×Ÿ ×‘××œ×™×¤×•×ª ×”××—×•×– ×‘-15 ×‘××¨×¥ 2025"

"×¤×¨×¡ ×œ××•×¨×” ××¦×˜×™×™× ×ª - ×”×’×‘' ×¨×—×œ ×›×”×Ÿ ×–×›×ª×” ×‘×¤×¨×¡ ××¦×˜×™×™× ×•×ª ×—×™× ×•×›×™×ª"

"×”×™×©×’ ×‘×©×—×™×™×” - ×”×ª×œ××™×“ ×™×•×¡×™ ×œ×•×™ ×©×‘×¨ ×©×™× ×‘×™×ª ×”×¡×¤×¨ ×‘××©×š 100 ××˜×¨ ×—×•×¤×©×™ ×‘-20/03"

**ğŸ’¡ × ×™×ª×Ÿ ×’× ×œ×”×•×¡×™×£:**
â€¢ ×§×™×©×•×¨ ×œ××××¨/×ª××•× ×”
â€¢ ×ª××¨×™×›×™ ×ª×¦×•×’×” (×¢×“ ××ª×™ ×œ×”×¦×™×’ ×‘×§×¨×•×¡×œ×”)`,
        })
      } else {
        aiLogger.logTypeSelection(userInput, false)
        return NextResponse.json({
          success: true,
          message: '×œ× ×”×‘× ×ª×™ ××ª ×”×‘×—×™×¨×” ×©×œ×š ğŸ˜•\n\n×× × ×‘×—×¨:\n1ï¸âƒ£ ××™×¨×•×¢\n2ï¸âƒ£ ×”×•×“×¢×” ×“×—×•×¤×”\n3ï¸âƒ£ ×”×“×’×©×”',
        })
      }
    }

    // Handle understanding check for complex messages (Round 1)
    if (action === 'understand_message') {
      const userMessage = messages[messages.length - 1]?.content || ''
      const startTime = Date.now()

      const requestMessages = [
        { role: 'system' as const, content: UNDERSTANDING_PROMPT },
        ...messages.map((msg: ChatMessage) => ({
          role: msg.role,
          content: msg.content,
        })),
      ]

      // Log GPT request with FULL details
      aiLogger.logGPTRequest({
        action: 'understand_message',
        userMessage,
        gptModel: AI_CONFIG.model,
        roundNumber: 1,
        context: undefined,
        metadata: {
          fullRequest: {
            model: AI_CONFIG.model,
            max_completion_tokens: AI_CONFIG.max_completion_tokens,
            messages: requestMessages, // FULL request messages
            timestamp: new Date().toISOString(),
          },
        },
      })

      const response = await openai.chat.completions.create({
        model: AI_CONFIG.model,
        max_completion_tokens: AI_CONFIG.max_completion_tokens,
        messages: requestMessages,
        // No function calling for understanding - just text response
      })

      const assistantMessage = response.choices[0].message
      const duration = Date.now() - startTime

      // Log GPT response with FULL details
      aiLogger.logGPTResponse({
        action: 'understand_message',
        responseType: 'text',
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0,
        cost: ((response.usage?.total_tokens || 0) * 0.000001), // GPT-5 Mini pricing
        roundNumber: 1,
        durationMs: duration,
        metadata: {
          fullResponse: {
            message: assistantMessage, // FULL assistant message
            finishReason: response.choices[0].finish_reason,
            usage: response.usage,
            timestamp: new Date().toISOString(),
          },
        },
      })

      return NextResponse.json({
        success: true,
        message: assistantMessage.content || '×œ× ×”×‘× ×ª×™. ×× × × ×¡×” ×©×•×‘.',
        understanding: assistantMessage.content, // Save for context in next round
      })
    }

    // Handle data extraction with function calling
    if (action === 'extract_data') {
      const userMessage = messages[messages.length - 1]?.content || ''
      const roundNumber = context ? 2 : 1
      const startTime = Date.now()

      // Use extraction prompt with optional context from understanding round
      const systemPrompt = getExtractionPrompt(context)

      const requestMessages = [
        { role: 'system' as const, content: systemPrompt },
        ...messages.map((msg: ChatMessage) => ({
          role: msg.role,
          content: msg.content,
        })),
      ]

      // Log GPT request with FULL details
      aiLogger.logGPTRequest({
        action: 'extract_data',
        userMessage,
        gptModel: AI_CONFIG.model,
        roundNumber,
        context,
        metadata: {
          fullRequest: {
            model: AI_CONFIG.model,
            max_completion_tokens: AI_CONFIG.max_completion_tokens,
            messages: requestMessages, // FULL request messages
            tools: AI_TOOLS, // FULL tools definition
            tool_choice: 'auto',
            systemPrompt, // FULL system prompt (with context if exists)
            hasContext: !!context,
            contextPreview: context?.substring(0, 200),
            timestamp: new Date().toISOString(),
          },
        },
      })

      const response = await openai.chat.completions.create({
        model: AI_CONFIG.model,
        max_completion_tokens: AI_CONFIG.max_completion_tokens,
        // Note: temperature is not included - GPT-5 Mini only supports default value (1)
        messages: requestMessages,
        tools: AI_TOOLS,
        // Use 'auto' instead of 'required' to allow AI to ask for clarification if needed
        // The explicit instructions in the user message guide the AI to call functions
        tool_choice: 'auto',
      })

      const assistantMessage = response.choices[0].message
      const duration = Date.now() - startTime

      // Determine response type
      const responseType = assistantMessage.tool_calls?.length
        ? 'function_call'
        : assistantMessage.content
        ? 'text'
        : 'error'

      // Extract function call details if present
      const toolCall = assistantMessage.tool_calls?.[0]
      const functionName = toolCall?.type === 'function' && 'function' in toolCall ? toolCall.function.name : undefined
      const functionArgs = functionName && toolCall?.type === 'function' && 'function' in toolCall
        ? JSON.parse(toolCall.function.arguments)
        : undefined

      // Log GPT response with FULL details
      aiLogger.logGPTResponse({
        action: 'extract_data',
        responseType,
        functionName,
        promptTokens: response.usage?.prompt_tokens || 0,
        completionTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0,
        cost: ((response.usage?.total_tokens || 0) * 0.000001), // GPT-5 Mini pricing
        roundNumber,
        durationMs: duration,
        metadata: {
          fullResponse: {
            message: assistantMessage, // FULL assistant message
            toolCalls: assistantMessage.tool_calls, // FULL tool calls
            functionName,
            functionArguments: functionArgs, // Parsed function arguments
            finishReason: response.choices[0].finish_reason,
            usage: response.usage,
            timestamp: new Date().toISOString(),
          },
        },
      })

      // Check if AI wants to call a function
      if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
        const toolCall = assistantMessage.tool_calls[0]
        if (toolCall.type === 'function') {
          const functionName = toolCall.function.name
          const functionArgs = JSON.parse(toolCall.function.arguments)

          // Log function call detection
          aiLogger.log({
            level: 'info',
            action: 'extract_data',
            responseType: 'function_call',
            functionName,
            metadata: {
              argsPreview: JSON.stringify(functionArgs).substring(0, 200),
            },
          })

          // Validate and return extracted data
          if (functionName === 'create_events' && validateEventsArgs(functionArgs)) {
            aiLogger.logValidation({
              action: 'extract_data',
              functionName,
              validationSuccess: true,
              extractedDataType: 'events',
            })
            return NextResponse.json({
              success: true,
              needsConfirmation: true,
              extractedData: {
                type: 'events',
                data: functionArgs.events,
              },
            })
          } else if (
          functionName === 'create_urgent_message' &&
          validateUrgentMessageArgs(functionArgs)
        ) {
          aiLogger.logValidation({
            action: 'extract_data',
            functionName,
            validationSuccess: true,
            extractedDataType: 'urgent_message',
          })
          return NextResponse.json({
            success: true,
            needsConfirmation: true,
            extractedData: {
              type: 'urgent_message',
              data: functionArgs,
            },
          })
          } else if (
          functionName === 'create_highlight' &&
          validateHighlightArgs(functionArgs)
        ) {
          aiLogger.logValidation({
            action: 'extract_data',
            functionName,
            validationSuccess: true,
            extractedDataType: 'highlight',
          })
          return NextResponse.json({
            success: true,
            needsConfirmation: true,
            extractedData: {
              type: 'highlight',
              data: functionArgs,
            },
          })
          } else {
            // Validation failed - return specific validation errors
            // Generate specific validation error messages
            const validationErrors: string[] = []

            if (functionName === 'create_events') {
              if (!functionArgs.events || functionArgs.events.length === 0) {
                validationErrors.push('×œ× × ××¦××• ××™×¨×•×¢×™× ×œ×™×™×¦×•×¨')
              } else {
                functionArgs.events.forEach((event: any, index: number) => {
                  if (!event.title) validationErrors.push(`××™×¨×•×¢ ${index + 1}: ×—×¡×¨ ×©×`)
                  if (!event.start_datetime) validationErrors.push(`××™×¨×•×¢ ${index + 1}: ×—×¡×¨ ×ª××¨×™×š`)
                  if (!event.title_ru) validationErrors.push(`××™×¨×•×¢ ${index + 1}: ×—×¡×¨ ×ª×¨×’×•× ×¨×•×¡×™`)
                })
              }
            } else if (functionName === 'create_urgent_message') {
              if (!functionArgs.title_he) validationErrors.push('×—×¡×¨×” ×›×•×ª×¨×ª ×‘×¢×‘×¨×™×ª')
              if (!functionArgs.title_ru) validationErrors.push('×—×¡×¨ ×ª×¨×’×•× ×¨×•×¡×™')
              if (!functionArgs.end_date) validationErrors.push('×—×¡×¨ ×ª××¨×™×š ×¡×™×•×')
            } else if (functionName === 'create_highlight') {
              if (!functionArgs.type) validationErrors.push('×—×¡×¨ ×¡×•×’ ×”×“×’×©×”')
              if (!functionArgs.icon) validationErrors.push('×—×¡×¨ ××™×™×§×•×Ÿ')
              if (!functionArgs.title_he || functionArgs.title_he.length < 2) validationErrors.push('×›×•×ª×¨×ª ×‘×¢×‘×¨×™×ª ×—×™×™×‘×ª ×œ×”×›×™×œ ×œ×¤×—×•×ª 2 ×ª×•×•×™×')
              if (!functionArgs.description_he || functionArgs.description_he.length < 10) validationErrors.push('×ª×™××•×¨ ×‘×¢×‘×¨×™×ª ×—×™×™×‘ ×œ×”×›×™×œ ×œ×¤×—×•×ª 10 ×ª×•×•×™×')
              if (!functionArgs.category_he || functionArgs.category_he.length < 2) validationErrors.push('×§×˜×’×•×¨×™×” ×‘×¢×‘×¨×™×ª ×—×™×™×‘×ª ×œ×”×›×™×œ ×œ×¤×—×•×ª 2 ×ª×•×•×™×')
            }

            // Log validation failure
            aiLogger.logValidation({
              action: 'extract_data',
              functionName,
              validationSuccess: false,
              validationErrors,
            })

            return NextResponse.json({
              success: false,
              error: '×©×’×™××” ×‘××™××•×ª ×”× ×ª×•× ×™× ×©×—×•×œ×¦×• ××”×”×•×“×¢×”',
              validationErrors,
            })
          }
        }
      }

      // AI responded with text (no function call) - this means it's asking for clarification
      // This is actually SUCCESS - AI is asking for missing information
      aiLogger.log({
        level: 'info',
        action: 'extract_data',
        responseType: 'text',
        userMessage: messages[messages.length - 1]?.content,
        metadata: {
          aiResponse: assistantMessage.content?.substring(0, 200),
          finishReason: response.choices[0].finish_reason,
          clarificationNeeded: true,
        },
      })

      return NextResponse.json({
        success: true,
        message: assistantMessage.content || '×œ× ×”×‘× ×ª×™. ×× × × ×¡×” ×©×•×‘.',
      })
    }

    return NextResponse.json({
      success: false,
      error: 'Invalid action',
    })
  } catch (error) {
    // Log error using AI logger with FULL details
    aiLogger.logError({
      action: 'extract_data', // Default to extract_data since most errors happen there
      errorMessage: error instanceof Error ? error.message : 'Unknown error',
      errorStack: error instanceof Error ? error.stack : undefined,
      metadata: {
        fullError: {
          name: error instanceof Error ? error.name : 'UnknownError',
          message: error instanceof Error ? error.message : String(error),
          stack: error instanceof Error ? error.stack : undefined,
          // Capture OpenAI API errors specifically
          openAIError: error && typeof error === 'object' && 'status' in error ? {
            status: (error as any).status,
            code: (error as any).code,
            type: (error as any).type,
            param: (error as any).param,
          } : undefined,
          timestamp: new Date().toISOString(),
        },
        requestBody, // Full request body
      },
    })

    console.error('[AI Assistant] Critical error:', error)
    console.error('[AI Assistant] Error details:', JSON.stringify(error, null, 2))

    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
      },
      { status: 500 }
    )
  }
}
